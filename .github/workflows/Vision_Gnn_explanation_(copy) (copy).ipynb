{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "16of9tir70M_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def position_1D(dimension,pos):\n",
        "    assert dimension%2==0\n",
        "    omega=np.arange(dimension//2,dtype=float)\n",
        "    #print(omega)\n",
        "    omega/=dimension/2.\n",
        "    omega=1./10000**omega\n",
        "    print(omega)\n",
        "    pos=pos.reshape(-1)\n",
        "    out=np.einsum('m,d->md',pos,omega)\n",
        "    #print(out)\n",
        "    Sin_out=np.sin(out)\n",
        "    Cos_out=np.cos(out)\n",
        "    embedding=np.concatenate([Sin_out,Cos_out],axis=1)\n",
        "    return embedding\n",
        "def position_2D(dimension,grid):\n",
        "    embed_H=position_1D(dimension//2,grid[0])\n",
        "    embed_W=position_1D(dimension//2,grid[1])\n",
        "    embeding=np.concatenate([embed_H,embed_W],axis=1)\n",
        "    return embeding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7f5HCzQC70NA",
        "outputId": "47c25039-148b-4846-affe-6dc2c95670a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grid_h:\n",
            "[0. 1. 2. 3. 4.]\n",
            "grid_w:\n",
            "[0. 1. 2. 3. 4.]\n",
            "After meshgrid:\n",
            "grid[0] (X coordinates):\n",
            "grid[1] (Y coordinates):\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [2. 2. 2. 2. 2.]\n",
            " [3. 3. 3. 3. 3.]\n",
            " [4. 4. 4. 4. 4.]]\n",
            "Stacked grid:\n",
            "[[[0. 1. 2. 3. 4.]\n",
            "  [0. 1. 2. 3. 4.]\n",
            "  [0. 1. 2. 3. 4.]\n",
            "  [0. 1. 2. 3. 4.]\n",
            "  [0. 1. 2. 3. 4.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 1. 1.]\n",
            "  [2. 2. 2. 2. 2.]\n",
            "  [3. 3. 3. 3. 3.]\n",
            "  [4. 4. 4. 4. 4.]]]\n",
            "Stacked grid shape: (2, 5, 5)\n",
            "[0.]\n",
            "[1.]\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]]\n",
            "pos [[ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid size (height and width of the grid)\n",
        "grid_size = 5\n",
        "\n",
        "# Create arrays representing the height and width of the grid\n",
        "grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "\n",
        "print(\"grid_h:\")\n",
        "print(grid_h)\n",
        "print(\"grid_w:\")\n",
        "print(grid_w)\n",
        "\n",
        "# Generate a coordinate grid\n",
        "grid = np.meshgrid(grid_w, grid_h)  # Here width goes first\n",
        "\n",
        "print(\"After meshgrid:\")\n",
        "print(\"grid[0] (X coordinates):\")\n",
        "#print(grid[0][0])  # X coordinates\n",
        "print(\"grid[1] (Y coordinates):\")\n",
        "print(grid[1])  # Y coordinates\n",
        "\n",
        "# Stack these two arrays along a new axis\n",
        "stacked_grid = np.stack(grid, axis=0)\n",
        "#stacked_grid=stacked_grid.reshape(2,1,5,5)\n",
        "print(\"Stacked grid:\")\n",
        "print(stacked_grid)\n",
        "\n",
        "print(\"Stacked grid shape:\", stacked_grid.shape)\n",
        "print('pos',position_1D(2,stacked_grid[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "position_1D(2,stacked_grid[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P3El1i5570NB",
        "outputId": "56a67730-ca02-4980-96e8-f374dd6198cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  1.        ],\n",
              "       [ 0.84147098,  0.54030231],\n",
              "       [ 0.90929743, -0.41614684],\n",
              "       [ 0.14112001, -0.9899925 ],\n",
              "       [-0.7568025 , -0.65364362],\n",
              "       [ 0.        ,  1.        ],\n",
              "       [ 0.84147098,  0.54030231],\n",
              "       [ 0.90929743, -0.41614684],\n",
              "       [ 0.14112001, -0.9899925 ],\n",
              "       [-0.7568025 , -0.65364362],\n",
              "       [ 0.        ,  1.        ],\n",
              "       [ 0.84147098,  0.54030231],\n",
              "       [ 0.90929743, -0.41614684],\n",
              "       [ 0.14112001, -0.9899925 ],\n",
              "       [-0.7568025 , -0.65364362],\n",
              "       [ 0.        ,  1.        ],\n",
              "       [ 0.84147098,  0.54030231],\n",
              "       [ 0.90929743, -0.41614684],\n",
              "       [ 0.14112001, -0.9899925 ],\n",
              "       [-0.7568025 , -0.65364362],\n",
              "       [ 0.        ,  1.        ],\n",
              "       [ 0.84147098,  0.54030231],\n",
              "       [ 0.90929743, -0.41614684],\n",
              "       [ 0.14112001, -0.9899925 ],\n",
              "       [-0.7568025 , -0.65364362]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "position_1D(2,stacked_grid[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ttnED0x370NB",
        "outputId": "dc875ff0-94ce-4fc6-cd1d-df1038449657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.]\n",
            "[1.]\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]]\n",
            "Embedding for Height:\n",
            "[[ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]]\n",
            "[0.]\n",
            "[1.]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]]\n",
            "\n",
            "Embedding for Width:\n",
            "[[ 0.          1.        ]\n",
            " [ 0.          1.        ]\n",
            " [ 0.          1.        ]\n",
            " [ 0.          1.        ]\n",
            " [ 0.          1.        ]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [ 0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362]\n",
            " [-0.7568025  -0.65364362]\n",
            " [-0.7568025  -0.65364362]\n",
            " [-0.7568025  -0.65364362]\n",
            " [-0.7568025  -0.65364362]]\n",
            "\n",
            "Final 2D Position Embedding:\n",
            "[[ 0.          1.          0.          1.        ]\n",
            " [ 0.84147098  0.54030231  0.          1.        ]\n",
            " [ 0.90929743 -0.41614684  0.          1.        ]\n",
            " [ 0.14112001 -0.9899925   0.          1.        ]\n",
            " [-0.7568025  -0.65364362  0.          1.        ]\n",
            " [ 0.          1.          0.84147098  0.54030231]\n",
            " [ 0.84147098  0.54030231  0.84147098  0.54030231]\n",
            " [ 0.90929743 -0.41614684  0.84147098  0.54030231]\n",
            " [ 0.14112001 -0.9899925   0.84147098  0.54030231]\n",
            " [-0.7568025  -0.65364362  0.84147098  0.54030231]\n",
            " [ 0.          1.          0.90929743 -0.41614684]\n",
            " [ 0.84147098  0.54030231  0.90929743 -0.41614684]\n",
            " [ 0.90929743 -0.41614684  0.90929743 -0.41614684]\n",
            " [ 0.14112001 -0.9899925   0.90929743 -0.41614684]\n",
            " [-0.7568025  -0.65364362  0.90929743 -0.41614684]\n",
            " [ 0.          1.          0.14112001 -0.9899925 ]\n",
            " [ 0.84147098  0.54030231  0.14112001 -0.9899925 ]\n",
            " [ 0.90929743 -0.41614684  0.14112001 -0.9899925 ]\n",
            " [ 0.14112001 -0.9899925   0.14112001 -0.9899925 ]\n",
            " [-0.7568025  -0.65364362  0.14112001 -0.9899925 ]\n",
            " [ 0.          1.         -0.7568025  -0.65364362]\n",
            " [ 0.84147098  0.54030231 -0.7568025  -0.65364362]\n",
            " [ 0.90929743 -0.41614684 -0.7568025  -0.65364362]\n",
            " [ 0.14112001 -0.9899925  -0.7568025  -0.65364362]\n",
            " [-0.7568025  -0.65364362 -0.7568025  -0.65364362]]\n"
          ]
        }
      ],
      "source": [
        "dimension = 4\n",
        "grid1= np.array([[[0, 1], [0, 1]], [[0, 0], [1, 1]]])  # shape (2, 2, 2)\n",
        "\n",
        "# Process the grid along the height (axis 0)\n",
        "embed_H=position_1D(dimension//2,grid[0])\n",
        "print(\"Embedding for Height:\")\n",
        "print(embed_H)\n",
        "\n",
        "# Process the grid along the width (axis 1)\n",
        "embed_W=position_1D(dimension//2,grid[1])\n",
        "print(\"\\nEmbedding for Width:\")\n",
        "print(embed_W)\n",
        "\n",
        "# Combine the height and width embeddings\n",
        "embedding=np.concatenate([embed_H,embed_W],axis=1)\n",
        "print(\"\\nFinal 2D Position Embedding:\")\n",
        "print(embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N_nl0sLv70NC",
        "outputId": "62784dd4-5e4d-49e6-f856-f8661d784b93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid1[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dryiKZ3a70NC"
      },
      "outputs": [],
      "source": [
        "def get_2d_position_from_start(embed_dim,grid_size):\n",
        "    grid_height=np.arange(grid_size,dtype=np.float32)\n",
        "    grid_width=np.arange(grid_size,dtype=np.float32)\n",
        "    grid=np.meshgrid(grid_width,grid_height)\n",
        "    grid=np.stack(grid,axis=0)\n",
        "    position=position_2D(embed_dim,grid)\n",
        "    return position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7-td7PsH70NC"
      },
      "outputs": [],
      "source": [
        "def relative_position(embed_dim,grid_size):\n",
        "    pos_embedding=get_2d_position_from_start(embed_dim,grid_size)\n",
        "    relative_position=2*np.matmul(pos_embedding,pos_embedding.transpose())/pos_embedding.shape[1]\n",
        "    return relative_position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8PRJ7M1S70NC",
        "outputId": "4d98bcc5-0839-4439-c7ed-47072b77a6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.]\n",
            "[1.]\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]]\n",
            "[0.]\n",
            "[1.]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 1.        ,  0.77015115,  0.29192658,  0.00500375,  0.17317819,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523,\n",
              "         0.00500375, -0.2248451 , -0.70306967, -0.9899925 , -0.82181806,\n",
              "         0.17317819, -0.05667066, -0.53489523, -0.82181806, -0.65364362],\n",
              "       [ 0.77015115,  1.        ,  0.77015115,  0.29192658,  0.00500375,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967,\n",
              "        -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967, -0.9899925 ,\n",
              "        -0.05667066,  0.17317819, -0.05667066, -0.53489523, -0.82181806],\n",
              "       [ 0.29192658,  0.77015115,  1.        ,  0.77015115,  0.29192658,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "        -0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684,\n",
              "        -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967,\n",
              "        -0.53489523, -0.05667066,  0.17317819, -0.05667066, -0.53489523],\n",
              "       [ 0.00500375,  0.29192658,  0.77015115,  1.        ,  0.77015115,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "        -0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773,\n",
              "        -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 ,\n",
              "        -0.82181806, -0.53489523, -0.05667066,  0.17317819, -0.05667066],\n",
              "       [ 0.17317819,  0.00500375,  0.29192658,  0.77015115,  1.        ,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "        -0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658,\n",
              "        -0.82181806, -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375,\n",
              "        -0.65364362, -0.82181806, -0.53489523, -0.05667066,  0.17317819],\n",
              "       [ 0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         1.        ,  0.77015115,  0.29192658,  0.00500375,  0.17317819,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523,\n",
              "         0.00500375, -0.2248451 , -0.70306967, -0.9899925 , -0.82181806],\n",
              "       [ 0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.77015115,  1.        ,  0.77015115,  0.29192658,  0.00500375,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967,\n",
              "        -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967, -0.9899925 ],\n",
              "       [ 0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "         0.29192658,  0.77015115,  1.        ,  0.77015115,  0.29192658,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "        -0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684,\n",
              "        -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967],\n",
              "       [-0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "         0.00500375,  0.29192658,  0.77015115,  1.        ,  0.77015115,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "        -0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773,\n",
              "        -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 ],\n",
              "       [-0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "         0.17317819,  0.00500375,  0.29192658,  0.77015115,  1.        ,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "        -0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658,\n",
              "        -0.82181806, -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375],\n",
              "       [ 0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         1.        ,  0.77015115,  0.29192658,  0.00500375,  0.17317819,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523],\n",
              "       [ 0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.77015115,  1.        ,  0.77015115,  0.29192658,  0.00500375,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967],\n",
              "       [-0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "         0.29192658,  0.77015115,  1.        ,  0.77015115,  0.29192658,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "        -0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684],\n",
              "       [-0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "         0.00500375,  0.29192658,  0.77015115,  1.        ,  0.77015115,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "        -0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773],\n",
              "       [-0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "         0.17317819,  0.00500375,  0.29192658,  0.77015115,  1.        ,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "        -0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658],\n",
              "       [ 0.00500375, -0.2248451 , -0.70306967, -0.9899925 , -0.82181806,\n",
              "         0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         1.        ,  0.77015115,  0.29192658,  0.00500375,  0.17317819,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066],\n",
              "       [-0.2248451 ,  0.00500375, -0.2248451 , -0.70306967, -0.9899925 ,\n",
              "         0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.77015115,  1.        ,  0.77015115,  0.29192658,  0.00500375,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ],\n",
              "       [-0.70306967, -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967,\n",
              "        -0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "         0.29192658,  0.77015115,  1.        ,  0.77015115,  0.29192658,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773],\n",
              "       [-0.9899925 , -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 ,\n",
              "        -0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "         0.00500375,  0.29192658,  0.77015115,  1.        ,  0.77015115,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231],\n",
              "       [-0.82181806, -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375,\n",
              "        -0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "         0.17317819,  0.00500375,  0.29192658,  0.77015115,  1.        ,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115],\n",
              "       [ 0.17317819, -0.05667066, -0.53489523, -0.82181806, -0.65364362,\n",
              "         0.00500375, -0.2248451 , -0.70306967, -0.9899925 , -0.82181806,\n",
              "         0.29192658,  0.06207773, -0.41614684, -0.70306967, -0.53489523,\n",
              "         0.77015115,  0.54030231,  0.06207773, -0.2248451 , -0.05667066,\n",
              "         1.        ,  0.77015115,  0.29192658,  0.00500375,  0.17317819],\n",
              "       [-0.05667066,  0.17317819, -0.05667066, -0.53489523, -0.82181806,\n",
              "        -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967, -0.9899925 ,\n",
              "         0.06207773,  0.29192658,  0.06207773, -0.41614684, -0.70306967,\n",
              "         0.54030231,  0.77015115,  0.54030231,  0.06207773, -0.2248451 ,\n",
              "         0.77015115,  1.        ,  0.77015115,  0.29192658,  0.00500375],\n",
              "       [-0.53489523, -0.05667066,  0.17317819, -0.05667066, -0.53489523,\n",
              "        -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 , -0.70306967,\n",
              "        -0.41614684,  0.06207773,  0.29192658,  0.06207773, -0.41614684,\n",
              "         0.06207773,  0.54030231,  0.77015115,  0.54030231,  0.06207773,\n",
              "         0.29192658,  0.77015115,  1.        ,  0.77015115,  0.29192658],\n",
              "       [-0.82181806, -0.53489523, -0.05667066,  0.17317819, -0.05667066,\n",
              "        -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375, -0.2248451 ,\n",
              "        -0.70306967, -0.41614684,  0.06207773,  0.29192658,  0.06207773,\n",
              "        -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,  0.54030231,\n",
              "         0.00500375,  0.29192658,  0.77015115,  1.        ,  0.77015115],\n",
              "       [-0.65364362, -0.82181806, -0.53489523, -0.05667066,  0.17317819,\n",
              "        -0.82181806, -0.9899925 , -0.70306967, -0.2248451 ,  0.00500375,\n",
              "        -0.53489523, -0.70306967, -0.41614684,  0.06207773,  0.29192658,\n",
              "        -0.05667066, -0.2248451 ,  0.06207773,  0.54030231,  0.77015115,\n",
              "         0.17317819,  0.00500375,  0.29192658,  0.77015115,  1.        ]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relative_position(4,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zL9KKG8O70ND"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ol4neFZA70ND",
        "outputId": "1d84bbc9-f316-462a-9492-b7a501a66be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/accl-ros/.local/lib/python3.10/site-packages (1.13.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: wheel in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7CvQLRrG70NE"
      },
      "outputs": [],
      "source": [
        "def pairwise_distance(x):\n",
        "    \"\"\"\n",
        "    Compute pairwise distance of a point cloud.\n",
        "    Args:\n",
        "        x: tensor (batch_size, num_points, num_dims)\n",
        "    Returns:\n",
        "        pairwise distance: (batch_size, num_points, num_points)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n",
        "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
        "        return x_square + x_inner + x_square.transpose(2, 1)\n",
        "\n",
        "\n",
        "def part_pairwise_distance(x, start_idx=0, end_idx=1):\n",
        "    \"\"\"\n",
        "    Compute pairwise distance of a point cloud.\n",
        "    Args:\n",
        "        x: tensor (batch_size, num_points, num_dims)\n",
        "    Returns:\n",
        "        pairwise distance: (batch_size, num_points, num_points)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x_part = x[:, start_idx:end_idx]\n",
        "        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n",
        "        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n",
        "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
        "        return x_square_part + x_inner + x_square.transpose(2, 1)\n",
        "\n",
        "\n",
        "def xy_pairwise_distance(x, y):\n",
        "    \"\"\"\n",
        "    Compute pairwise distance of a point cloud.\n",
        "    Args:\n",
        "        x: tensor (batch_size, num_points, num_dims)\n",
        "    Returns:\n",
        "        pairwise distance: (batch_size, num_points, num_points)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        xy_inner = -2*torch.matmul(x, y.transpose(2, 1))\n",
        "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
        "        y_square = torch.sum(torch.mul(y, y), dim=-1, keepdim=True)\n",
        "        return x_square + xy_inner + y_square.transpose(2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R1EQ-iA-70NE"
      },
      "outputs": [],
      "source": [
        "def dense_knn_matrix(x,k=16,relative_position=None):\n",
        "    with torch.no_grad():\n",
        "        x=x.transpose(2,1).squeeze(-1)\n",
        "        batch_size,n_points,n_dims=x.shape\n",
        "        n_part=10000\n",
        "\n",
        "        if n_points>n_part:\n",
        "            nn_idsx_list=[]\n",
        "            groups=math.ceil(n_points/n_part)\n",
        "            for i in range (groups):\n",
        "                start_idx=n_part*i\n",
        "                end_idx=min(n_points,n_part*(i+1))\n",
        "                distance=part_pairwise_distance(x.detach(),start_idx,end_idx)\n",
        "                if relative_position is not None:\n",
        "                    distance+=relative_position[:,start_idx:end_idx]\n",
        "                _,nn_idx_part=torch.topk(-distance,k=k)\n",
        "                nn_idx_list+=[nn.idx_list]\n",
        "            nn_idx=torch.cat(nn_idsx_list,dim=1)\n",
        "        else:\n",
        "            distance=pairwise_distance(x.detach())\n",
        "            if relative_position is not None:\n",
        "                distance+=relative_position\n",
        "                _,nn_idx=torch.topk(-distance,k=k)\n",
        "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
        "        return torch.stack((nn_idx,center_idx),dim=0)\n",
        "def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n",
        "\n",
        "    with torch.no_grad():\n",
        "            x = x.transpose(2, 1).squeeze(-1)\n",
        "            y = y.transpose(2, 1).squeeze(-1)\n",
        "            batch_size, n_points, n_dims = x.shape\n",
        "            dist = xy_pairwise_distance(x.detach(), y.detach())\n",
        "            if relative_pos is not None:\n",
        "                dist += relative_pos\n",
        "            _, nn_idx = torch.topk(-dist, k=k)\n",
        "            center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
        "    return torch.stack((nn_idx, center_idx), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "egc2zPEz70NF"
      },
      "outputs": [],
      "source": [
        "class DenseDilated(nn.Module):\n",
        "    def __init__(self,k=9,dilation=9,stochastic=False,epsilon=0.0):\n",
        "        super(DenseDilated, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.stochastic = stochastic\n",
        "        self.epsilon = epsilon\n",
        "        self.k = k\n",
        "    def forward(self,edge_index):\n",
        "        if self.stochastic:\n",
        "            if torch.rand(1) < self.epsilon and self.training:\n",
        "                num=self.k*self.dilation\n",
        "                randnum=torch.randperm(num)[:self.k]\n",
        "                edge_index=edge_index[:,:,:,randnum]\n",
        "            else:\n",
        "                edge_index=edge_index[:,:,:,::self.dilation]\n",
        "        else:\n",
        "            edge_index=edge_index[:,:,:,::self.dilation]\n",
        "        return edge_index\n",
        "\n",
        "class DenseDilatedKnnGraph(nn.Module):\n",
        "    \"\"\"\n",
        "    Find the neighbors' indices based on dilated knn\n",
        "    \"\"\"\n",
        "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
        "        super(DenseDilatedKnnGraph, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.stochastic = stochastic\n",
        "        self.epsilon = epsilon\n",
        "        self.k = k\n",
        "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
        "\n",
        "    def forward(self, x, y=None, relative_pos=None):\n",
        "        if y is not None:\n",
        "            #### normalize\n",
        "            x = F.normalize(x, p=2.0, dim=1)\n",
        "            y = F.normalize(y, p=2.0, dim=1)\n",
        "            ####\n",
        "            edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n",
        "        else:\n",
        "            #### normalize\n",
        "            x = F.normalize(x, p=2.0, dim=1)\n",
        "            ####\n",
        "            edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n",
        "        return self._dilated(edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IGi2JdQn70NF"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OQyB5HM470NF"
      },
      "outputs": [],
      "source": [
        "def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n",
        "    # activation layer\n",
        "\n",
        "    act = act.lower()\n",
        "    if act == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act == 'leakyrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    elif act == 'gelu':\n",
        "        layer = nn.GELU()\n",
        "    elif act == 'hswish':\n",
        "        layer = nn.Hardswish(inplace)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
        "    return layer\n",
        "def norm_layer(norm, nc):\n",
        "    # normalization layer 2d\n",
        "    norm = norm.lower()\n",
        "    if norm == 'batch':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "    elif norm == 'instance':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n",
        "    return layer\n",
        "class BasicConv(Seq):\n",
        "    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n",
        "        m = []\n",
        "        for i in range(1, len(channels)):\n",
        "            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n",
        "            if norm is not None and norm.lower() != 'none':\n",
        "                m.append(norm_layer(norm, channels[-1]))\n",
        "            if act is not None and act.lower() != 'none':\n",
        "                m.append(act_layer(act))\n",
        "            if drop > 0:\n",
        "                m.append(nn.Dropout2d(drop))\n",
        "\n",
        "        super(BasicConv, self).__init__(*m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "def batched_index_select(x, idx):\n",
        "    r\"\"\"fetches neighbors features from a given neighbor idx\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): input feature Tensor\n",
        "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n",
        "        idx (Tensor): edge_idx\n",
        "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n",
        "    Returns:\n",
        "        Tensor: output neighbors features\n",
        "            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n",
        "    \"\"\"\n",
        "    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n",
        "    _, num_vertices, k = idx.shape\n",
        "    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n",
        "    idx = idx + idx_base\n",
        "    idx = idx.contiguous().view(-1)\n",
        "\n",
        "    x = x.transpose(2, 1)\n",
        "    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "a=torch.arange(0, 5).view(-1, 1, 1)\n",
        "a.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8UFFQswP70NF"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "class BasicConv(Seq):\n",
        "    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n",
        "        m = []\n",
        "        for i in range(1, len(channels)):\n",
        "            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n",
        "            if norm is not None and norm.lower() != 'none':\n",
        "                m.append(norm_layer(norm, channels[-1]))\n",
        "            if act is not None and act.lower() != 'none':\n",
        "                m.append(act_layer(act))\n",
        "            if drop > 0:\n",
        "                m.append(nn.Dropout2d(drop))\n",
        "\n",
        "        super(BasicConv, self).__init__(*m)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JrP02a0770NF"
      },
      "outputs": [],
      "source": [
        "class Max_relative(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
        "        super(Max_relative,self).__init__()\n",
        "        self.conv1=BasicConv([in_channels*2,out_channels],act, norm, bias)\n",
        "    def forward(self,x,edge_index,y=None):\n",
        "        x_i=batched_index_select(x,edge_index[1])\n",
        "        if y is not None:\n",
        "            x_j = batched_index_select(y, edge_index[0])\n",
        "        else:\n",
        "            x_j = batched_index_select(x, edge_index[0])\n",
        "        x_j,_=torch.max(x_j-x_i,-1, keepdim=True)\n",
        "        b, c, n, _ = x.shape\n",
        "        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, _)\n",
        "        return self.conv1(x)\n",
        "\n",
        "class EdgeConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Edge convolution layer (with activation, batch normalization) for dense data type\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
        "        super(EdgeConv2d, self).__init__()\n",
        "        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n",
        "\n",
        "    def forward(self, x, edge_index, y=None):\n",
        "        x_i = batched_index_select(x, edge_index[1])\n",
        "        if y is not None:\n",
        "            x_j = batched_index_select(y, edge_index[0])\n",
        "        else:\n",
        "            x_j = batched_index_select(x, edge_index[0])\n",
        "        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n",
        "        return max_value\n",
        "\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    \"\"\"\n",
        "    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)\n",
        "        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)\n",
        "\n",
        "    def forward(self, x, edge_index, y=None):\n",
        "        if y is not None:\n",
        "            x_j = batched_index_select(y, edge_index[0])\n",
        "        else:\n",
        "            x_j = batched_index_select(x, edge_index[0])\n",
        "        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)\n",
        "        return self.nn2(torch.cat([x, x_j], dim=1))\n",
        "\n",
        "\n",
        "class GINConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
        "        super(GINConv2d, self).__init__()\n",
        "        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)\n",
        "        eps_init = 0.0\n",
        "        self.eps = nn.Parameter(torch.Tensor([eps_init]))\n",
        "\n",
        "    def forward(self, x, edge_index, y=None):\n",
        "        if y is not None:\n",
        "            x_j = batched_index_select(y, edge_index[0])\n",
        "        else:\n",
        "            x_j = batched_index_select(x, edge_index[0])\n",
        "        x_j = torch.sum(x_j, -1, keepdim=True)\n",
        "        return self.nn((1 + self.eps) * x + x_j)\n",
        "\n",
        "\n",
        "class GraphConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Static graph convolution layer\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):\n",
        "        super(GraphConv2d, self).__init__()\n",
        "        if conv == 'edge':\n",
        "            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)\n",
        "        elif conv == 'mr':\n",
        "            self.gconv = Max_relative(in_channels, out_channels, act, norm, bias)\n",
        "        elif conv == 'sage':\n",
        "            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)\n",
        "        elif conv == 'gin':\n",
        "            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)\n",
        "        else:\n",
        "            raise NotImplementedError('conv:{} is not supported'.format(conv))\n",
        "\n",
        "    def forward(self, x, edge_index, y=None):\n",
        "        return self.gconv(x, edge_index, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UVjBlCEX70NF"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FW7aC7r070NG"
      },
      "outputs": [],
      "source": [
        "class Dynamic_graph(GraphConv2d):\n",
        "    def __init__(self,in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', act='relu',\n",
        "                 norm=None, bias=True, stochastic=False, epsilon=0.0, r=1):\n",
        "        super(Dynamic_graph,self).__init__(in_channels,out_channels,conv,act,norm,bias)\n",
        "        self.k=kernel_size\n",
        "        self.d=dilation\n",
        "        self.r=r\n",
        "        self.knn_graph=DenseDilatedKnnGraph(kernel_size,dilation,stochastic,epsilon)\n",
        "    def forward(self,x,relative_position=None):\n",
        "        B,C,H,W=x.shape\n",
        "        y=None\n",
        "        if self.r > 1:\n",
        "            y = F.avg_pool2d(x, self.r, self.r)\n",
        "            y = y.reshape(B, C, -1, 1).contiguous()\n",
        "        x=x.reshape(B,C,-1,1).contiguous()\n",
        "        edge_index=self.knn_graph(x,y,relative_position)\n",
        "        x=super(Dynamic_graph,self).forward(x,edge_index,y)\n",
        "        return x.reshape(B, -1, H, W).contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-1GgWeSL70NG",
        "outputId": "e4a1a587-aa1f-47e5-a77c-a84427827134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rwightman/pytorch-image-models.git\n",
            "  Cloning https://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-rmun1hb5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-rmun1hb5\n",
            "  Resolved https://github.com/rwightman/pytorch-image-models.git to commit d3ebdcfd938baf048ef784210de127013320cde3\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from timm==0.9.8.dev0) (0.3.1)\n",
            "Requirement already satisfied: torchvision in /home/accl-ros/.local/lib/python3.10/site-packages (from timm==0.9.8.dev0) (0.14.1)\n",
            "Requirement already satisfied: pyyaml in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from timm==0.9.8.dev0) (6.0)\n",
            "Requirement already satisfied: torch>=1.7 in /home/accl-ros/.local/lib/python3.10/site-packages (from timm==0.9.8.dev0) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from timm==0.9.8.dev0) (0.16.4)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.8.dev0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.8.dev0) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.8.dev0) (4.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.8.dev0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/accl-ros/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.8.dev0) (8.5.0.96)\n",
            "Requirement already satisfied: setuptools in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm==0.9.8.dev0) (65.6.3)\n",
            "Requirement already satisfied: wheel in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm==0.9.8.dev0) (0.37.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/accl-ros/.local/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.8.dev0) (4.64.1)\n",
            "Requirement already satisfied: fsspec in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.8.dev0) (2023.1.0)\n",
            "Requirement already satisfied: filelock in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.8.dev0) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.8.dev0) (23.1)\n",
            "Requirement already satisfied: requests in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.8.dev0) (2.31.0)\n",
            "Requirement already satisfied: numpy in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from torchvision->timm==0.9.8.dev0) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from torchvision->timm==0.9.8.dev0) (9.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.8.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.8.dev0) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.8.dev0) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.8.dev0) (2.0.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ujFG89NZ70NG"
      },
      "outputs": [],
      "source": [
        "from timm.models.layers import DropPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p8TZhsaf70NG"
      },
      "outputs": [],
      "source": [
        "class  Grapher(nn.Module):\n",
        "    def __init__(self,in_channels, kernel_size=9, dilation=1, conv='edge', act='relu', norm=None,\n",
        "                 bias=True,  stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False) :\n",
        "        super(Grapher,self).__init__()\n",
        "        self.channels = in_channels\n",
        "        self.n = n\n",
        "        self.r = r\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "        self.graph_conv = Dynamic_graph(in_channels, in_channels * 2, kernel_size, dilation, conv,\n",
        "                              act, norm, bias, stochastic, epsilon, r)\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 2, in_channels, 1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.relative_pos = None\n",
        "        if relative_pos :\n",
        "            print('using relative pos')\n",
        "            relative_pos_tensor=torch.from_numpy(np.float32(relative_position(in_channels,int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n",
        "            relative_pos_tensor = F.interpolate(\n",
        "                    relative_pos_tensor, size=(n, n//(r*r)), mode='bicubic', align_corners=False)\n",
        "            self.relative_pos=nn.Parameter(-relative_pos_tensor.squeeze(1),requires_grad=False)\n",
        "    def _get_relative_pos(self, relative_pos, H, W):\n",
        "        if relative_pos is None or H * W == self.n:\n",
        "            return relative_pos\n",
        "        else:\n",
        "            N = H * W\n",
        "            N_reduced = N // (self.r * self.r)\n",
        "            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode=\"bicubic\").squeeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _tmp = x\n",
        "        x = self.fc1(x)\n",
        "        B, C, H, W = x.shape\n",
        "        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n",
        "        x = self.graph_conv(x, relative_pos)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop_path(x) + _tmp\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Zl0T5wTR70NG"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act='relu', drop_path=0.0):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(hidden_features),\n",
        "        )\n",
        "        self.act = act_layer(act)\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(out_features),\n",
        "        )\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KqUlRTfV70NG"
      },
      "outputs": [],
      "source": [
        "class Stem(nn.Module):\n",
        "    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, out_dim//2, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_dim//2),\n",
        "            act_layer(act),\n",
        "            nn.Conv2d(out_dim//2, out_dim, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "            act_layer(act),\n",
        "            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        return x\n",
        "class Downsample(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=3, out_dim=768):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G-cj1JY170NG"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Sequential as Seq\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.helpers import load_pretrained\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 6, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    'vig_224_gelu': _cfg(\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'vig_b_224_gelu': _cfg(\n",
        "        crop_pct=0.95, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "}\n",
        "class DeepGCN(torch.nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(DeepGCN, self).__init__()\n",
        "        print(opt)\n",
        "        k = opt.k\n",
        "        act = opt.act\n",
        "        norm = opt.norm\n",
        "        bias = opt.bias\n",
        "        epsilon = opt.epsilon\n",
        "        stochastic = opt.use_stochastic\n",
        "        conv = opt.conv\n",
        "        emb_dims = opt.emb_dims\n",
        "        drop_path = opt.drop_path\n",
        "\n",
        "        blocks = opt.blocks\n",
        "        self.n_blocks=sum(blocks)\n",
        "        channels = opt.channels\n",
        "        reduce_ratios = [4, 2, 1, 1]\n",
        "        drop_path_rate=[x.item() for x in torch.linspace(0,drop_path,self.n_blocks)]\n",
        "        num_knn=[int(x.item()) for x in torch.linspace(k,k,self.n_blocks) ]\n",
        "        max_dilation=49//max(num_knn)\n",
        "        self.stem=Stem(out_dim=channels[0],act=act)\n",
        "        self.pos_embed=nn.Parameter(torch.zeros(1,channels[0],224//4,224//4))\n",
        "\n",
        "        HW=224 // 4 * 224 // 4\n",
        "        self.backbone=nn.ModuleList([])\n",
        "        idx=0\n",
        "        for i in range (len(blocks)):\n",
        "            if i>0:\n",
        "                self.backbone.append(Downsample(channels[i-1],channels[i]))\n",
        "                HW=HW//4\n",
        "            for j in range (blocks[i]):\n",
        "                self.backbone.append (Seq(Grapher(channels[i],num_knn[idx],min(idx//4+1,max_dilation),conv, act, norm,\n",
        "                                    bias, stochastic, epsilon, reduce_ratios[i], n=HW, drop_path=drop_path_rate[idx],\n",
        "                                    relative_pos=True),\n",
        "                          FFN(channels[i], channels[i] * 4, act=act, drop_path=drop_path_rate[idx])\n",
        "                         ))\n",
        "                idx+=1\n",
        "\n",
        "        self.backbone = Seq(*self.backbone)\n",
        "\n",
        "\n",
        "        self.prediction = Seq(nn.Conv2d(channels[-1], 1024, 1, bias=True),\n",
        "                              nn.BatchNorm2d(1024),\n",
        "                              act_layer(act),\n",
        "                              nn.Dropout(opt.dropout),\n",
        "                              nn.Conv2d(1024, opt.n_classes, 1, bias=True))\n",
        "        self.model_init()\n",
        "    def model_init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "                m.weight.requires_grad = True\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "                    m.bias.requires_grad = True\n",
        "    def forward(self,inputs):\n",
        "        x=self.stem(inputs)+self.pos_embed\n",
        "        B, C, H, W = x.shape\n",
        "        for i in range(len(self.backbone)):\n",
        "            x = self.backbone[i](x)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        return self.prediction(x).squeeze(-1).squeeze(-1)\n",
        "@register_model\n",
        "def pvig_ti_224_gelu(pretrained=False, **kwargs):\n",
        "    class OptInit:\n",
        "        def __init__(self, num_classes=6, drop_path_rate=0.3, **kwargs):\n",
        "                self.k = 9 # neighbor num (default:9)\n",
        "                self.conv = 'mr' # graph conv layer {edge, mr}\n",
        "                self.act = 'gelu' # activation layer {relu, prelu, leakyrelu, gelu, hswish}\n",
        "                self.norm = 'batch' # batch or instance normalization {batch, instance}\n",
        "                self.bias = True # bias of conv layer True or False\n",
        "                self.dropout = 0.3 # dropout rate\n",
        "                self.use_dilation = True # use dilated knn or not\n",
        "                self.epsilon = 0.2 # stochastic epsilon for gcn\n",
        "                self.use_stochastic = False # stochastic for gcn, True or False\n",
        "                self.drop_path = drop_path_rate\n",
        "                self.blocks = [2,2,6,2] # number of basic blocks in the backbone\n",
        "                self.channels = [80, 160, 400, 640]  # number of channels of deep features\n",
        "                self.n_classes = num_classes # Dimension of out_channels\n",
        "                self.emb_dims = 1024 # Dimension of embeddings\n",
        "\n",
        "    opt = OptInit(**kwargs)\n",
        "    model = DeepGCN(opt)\n",
        "    model.default_cfg = default_cfgs['vig_224_gelu']\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bBlQyiwc70NH"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "y8_eICYf70NH"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BKhm1oan70NH",
        "outputId": "f35d5242-53b8-42d9-f59d-66f9aae08c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  6% |  4% |\n",
            "|  1 |  0% |  0% |\n",
            "Initial GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  6% |  4% |\n",
            "|  1 |  0% |  0% |\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'cuda' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGPU Usage after emptying the cache\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     gpu_usage()\n\u001b[0;32m---> 16\u001b[0m free_gpu_cache()\n",
            "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mfree_gpu_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m gpu_usage()\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> 9\u001b[0m cuda\u001b[39m.\u001b[39mselect_device(\u001b[39m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m cuda\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     11\u001b[0m cuda\u001b[39m.\u001b[39mselect_device(\u001b[39m0\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cuda' is not defined"
          ]
        }
      ],
      "source": [
        "from GPUtil import showUtilization as gpu_usage\n",
        "gpu_usage()\n",
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "free_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "J0_QBjVM70NH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxKd_pOY70NH",
        "outputId": "8a3d21f1-ff41-4446-f378-bb3d9344ea4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgKy6ojx70NH",
        "outputId": "7459c34d-d2af-45ae-d70a-682db56f86b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "B1W9vY9E70NH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data augmentation transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Create train and test datasets\n",
        "path = \"/home/accl/Desktop/borescope-adr-lm2500-data-develop/preprocessed\"\n",
        "train_dataset = ImageFolder(root='/home/accl-ros/wo_Dup/train/', transform=data_transforms)\n",
        "test_dataset = ImageFolder(root='/home/accl-ros/wo_Dup/test/', transform=data_transforms)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=30, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kZNL40ym70NI",
        "outputId": "41236258-6a6b-4023-fa6e-3be32c5401e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "X2_RgKh470NI",
        "outputId": "e640cf20-af9a-4e4b-d6eb-b7a54e5c71d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function check_balance at 0x7f730d7d60e0>\n"
          ]
        }
      ],
      "source": [
        "def check_balance(train_loader):\n",
        "    class_count = {}\n",
        "    for _, labels in train_loader:\n",
        "        for label in labels:\n",
        "            if label.item() not in class_count:\n",
        "                class_count[label.item()] = 0\n",
        "            class_count[label.item()] += 1\n",
        "    print(check_balance)\n",
        "    return(class_count)\n",
        "\n",
        "# Assume you have a DataLoader object named train_loader\n",
        "class_count=check_balance(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cAAQaYwu70NI",
        "outputId": "41e31638-c1a9-45ff-bf05-5d4fc30fe8b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{2: 955, 0: 447, 3: 662, 1: 667, 4: 1053, 5: 101}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Rr_nb3Hx70NI",
        "outputId": "0fc1018f-6425-484d-9631-2693c5b01aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (1.3.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (4.8.0.74)\n",
            "Requirement already satisfied: PyYAML in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (6.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (0.21.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from albumentations) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.7.18)\n",
            "Requirement already satisfied: imageio>=2.27 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\n",
            "Requirement already satisfied: networkx>=2.8 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
            "Requirement already satisfied: packaging>=21 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (23.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (9.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/accl-ros/anaconda3/envs/OCAST_project/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "X9W4ATlc70NI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8mhvM91R70NJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define data augmentation transformations for imbalance mitigation\n",
        "imbalance_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Set the target number of samples per class\n",
        "target_samples_per_class = max(class_count.values())\n",
        "\n",
        "# Separate the indices of samples from each class\n",
        "indices_per_class = {i: [] for i in class_count.keys()}\n",
        "for idx, (_, label) in enumerate(train_dataset):\n",
        "    indices_per_class[label].append(idx)\n",
        "\n",
        "# Initialize a list to hold the balanced datasets\n",
        "datasets = []\n",
        "\n",
        "# Iterate over each class\n",
        "for class_label, indices in indices_per_class.items():\n",
        "    # Calculate the number of extra samples needed to reach the target\n",
        "    extra_samples = target_samples_per_class - len(indices)\n",
        "\n",
        "    # Create a new subset for the class\n",
        "    class_subset = Subset(train_dataset, indices)\n",
        "\n",
        "    # Create transformations for data augmentation\n",
        "    class_subset.dataset.transform = imbalance_transforms\n",
        "\n",
        "    # Duplicate the subset the necessary number of times\n",
        "    class_datasets = [class_subset] * (1 + extra_samples // len(indices))\n",
        "\n",
        "    # If necessary, add some extra samples to reach the target exactly\n",
        "    if extra_samples % len(indices) != 0:\n",
        "        extra_subset = Subset(class_subset, range(extra_samples % len(indices)))\n",
        "        class_datasets.append(extra_subset)\n",
        "\n",
        "    # Concatenate the class datasets together\n",
        "    class_dataset = ConcatDataset(class_datasets)\n",
        "\n",
        "    # Add the class dataset to the list of datasets\n",
        "    datasets.append(class_dataset)\n",
        "\n",
        "# Concatenate all the class datasets together\n",
        "balanced_dataset = ConcatDataset(datasets)\n",
        "length = len(balanced_dataset)\n",
        "train_proportion = 0.8\n",
        "train_len = int(length * train_proportion)\n",
        "valid_len = length - train_len\n",
        "balanced_train_data, balanced_valid_data = random_split(balanced_dataset, [train_len, valid_len])\n",
        "balanced_train_loader = DataLoader(balanced_train_data, batch_size=190, shuffle=True)\n",
        "balanced_valid_loader = DataLoader(balanced_valid_data, batch_size=100, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "oDXueENy70NJ",
        "outputId": "6c80c90b-01f2-4563-c1a6-43677001d138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(balanced_train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uIU6UzfB70NJ",
        "outputId": "249f08b7-fd5c-47fc-82f7-3d6175c33e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function check_balance at 0x7f730d7d60e0>\n"
          ]
        }
      ],
      "source": [
        "f=check_balance(balanced_train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 GPUs!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_Z53JOV470NJ",
        "outputId": "b244994f-83be-412a-af5e-e80c10c8c03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.pvig_ti_224_gelu.<locals>.OptInit object at 0x7f730da5d090>\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 6.30957344e-01 3.98107171e-01 ... 3.98107171e-04\n",
            "  2.51188643e-04 1.58489319e-04]\n",
            " [2.00000000e+00 1.26191469e+00 7.96214341e-01 ... 7.96214341e-04\n",
            "  5.02377286e-04 3.16978638e-04]\n",
            " ...\n",
            " [5.30000000e+01 3.34407393e+01 2.10996800e+01 ... 2.10996800e-02\n",
            "  1.33129981e-02 8.39993392e-03]\n",
            " [5.40000000e+01 3.40716966e+01 2.14977872e+01 ... 2.14977872e-02\n",
            "  1.35641867e-02 8.55842324e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 6.30957344e-01 3.98107171e-01 ... 3.98107171e-04\n",
            "  2.51188643e-04 1.58489319e-04]\n",
            " [2.00000000e+00 1.26191469e+00 7.96214341e-01 ... 7.96214341e-04\n",
            "  5.02377286e-04 3.16978638e-04]\n",
            " ...\n",
            " [5.30000000e+01 3.34407393e+01 2.10996800e+01 ... 2.10996800e-02\n",
            "  1.33129981e-02 8.39993392e-03]\n",
            " [5.40000000e+01 3.40716966e+01 2.14977872e+01 ... 2.14977872e-02\n",
            "  1.35641867e-02 8.55842324e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 7.94328235e-01 6.30957344e-01 ... 1.99526231e-04\n",
            "  1.58489319e-04 1.25892541e-04]\n",
            " [2.00000000e+00 1.58865647e+00 1.26191469e+00 ... 3.99052463e-04\n",
            "  3.16978638e-04 2.51785082e-04]\n",
            " ...\n",
            " [2.50000000e+01 1.98582059e+01 1.57739336e+01 ... 4.98815579e-03\n",
            "  3.96223298e-03 3.14731353e-03]\n",
            " [2.60000000e+01 2.06525341e+01 1.64048910e+01 ... 5.18768202e-03\n",
            "  4.12072230e-03 3.27320607e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 7.94328235e-01 6.30957344e-01 ... 1.99526231e-04\n",
            "  1.58489319e-04 1.25892541e-04]\n",
            " [2.00000000e+00 1.58865647e+00 1.26191469e+00 ... 3.99052463e-04\n",
            "  3.16978638e-04 2.51785082e-04]\n",
            " ...\n",
            " [2.50000000e+01 1.98582059e+01 1.57739336e+01 ... 4.98815579e-03\n",
            "  3.96223298e-03 3.14731353e-03]\n",
            " [2.60000000e+01 2.06525341e+01 1.64048910e+01 ... 5.18768202e-03\n",
            "  4.12072230e-03 3.27320607e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.44060876e-01 8.91250938e-01 ... 1.18850223e-04\n",
            "  1.12201845e-04 1.05925373e-04]\n",
            " [2.00000000e+00 1.88812175e+00 1.78250188e+00 ... 2.37700445e-04\n",
            "  2.24403691e-04 2.11850745e-04]\n",
            " ...\n",
            " [4.00000000e+00 3.77624351e+00 3.56500375e+00 ... 4.75400891e-04\n",
            "  4.48807382e-04 4.23701490e-04]\n",
            " [5.00000000e+00 4.72030438e+00 4.45625469e+00 ... 5.94251114e-04\n",
            "  5.61009227e-04 5.29626863e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "using relative pos\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.44060876e-01 8.91250938e-01 ... 1.18850223e-04\n",
            "  1.12201845e-04 1.05925373e-04]\n",
            " [2.00000000e+00 1.88812175e+00 1.78250188e+00 ... 2.37700445e-04\n",
            "  2.24403691e-04 2.11850745e-04]\n",
            " ...\n",
            " [4.00000000e+00 3.77624351e+00 3.56500375e+00 ... 4.75400891e-04\n",
            "  4.48807382e-04 4.23701490e-04]\n",
            " [5.00000000e+00 4.72030438e+00 4.45625469e+00 ... 5.94251114e-04\n",
            "  5.61009227e-04 5.29626863e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m min_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(balanced_train_loader, \u001b[39m0\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:240\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:240\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:705\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[39m    img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m    PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mhflip(img)\n\u001b[1;32m    706\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:652\u001b[0m, in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    650\u001b[0m     _log_api_usage_once(hflip)\n\u001b[1;32m    651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 652\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mhflip(img)\n\u001b[1;32m    654\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mhflip(img)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:58\u001b[0m, in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mtranspose(_pil_constants\u001b[39m.\u001b[39;49mFLIP_LEFT_RIGHT)\n",
            "File \u001b[0;32m~/anaconda3/envs/OCAST_project/lib/python3.10/site-packages/PIL/Image.py:2728\u001b[0m, in \u001b[0;36mImage.transpose\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2718\u001b[0m \u001b[39mTranspose image (flip or rotate in 90 degree steps)\u001b[39;00m\n\u001b[1;32m   2719\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[39m:returns: Returns a flipped or rotated copy of this image.\u001b[39;00m\n\u001b[1;32m   2725\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m-> 2728\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mtranspose(method))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "model = pvig_ti_224_gelu(num_classes=6).to(device)\n",
        "if torch.cuda.device_count() > 1:\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(450):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    patience=10\n",
        "    stop_count = 0\n",
        "    min_valid_loss = float('inf')\n",
        "    model.train()\n",
        "\n",
        "    for i, data in enumerate(balanced_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 25== 24:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}%')\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    for i, data in enumerate(balanced_valid_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        valid_loss += loss.item()\n",
        "    valid_loss /= len(balanced_valid_loader.dataset)\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 500, valid_loss))\n",
        "\n",
        "    # Check for Early Stopping\n",
        "    if valid_loss < min_valid_loss:\n",
        "        min_valid_loss = valid_loss\n",
        "        stop_count = 0\n",
        "    else:\n",
        "        stop_count += 1\n",
        "        if stop_count >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "print('Finished Training')\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, 'model_checkpoint_ocast.pth')\n",
        "\n",
        "\n",
        "# Now let's plot the training loss and accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, 'r-', label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, 'g-', label='Train Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlSOnu7k70NJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Let's assume that you have a trained model called `model`\n",
        "\n",
        "# Saving the model\n",
        "with open('model_ocast.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Loading the model\n",
        "#with open('model.pkl', 'rb') as f:\n",
        "    #loaded_model = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, 'full_model_ocast.pth')\n",
        "model_l = torch.load('full_model_ocast.pth')\n",
        "# Set the model to evaluation mod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_l = torch.load('full_model_ocast.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchsummary\n",
            "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.5.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install torchsummary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 40, 112, 112]           1,120\n",
            "       BatchNorm2d-2         [-1, 40, 112, 112]              80\n",
            "              GELU-3         [-1, 40, 112, 112]               0\n",
            "            Conv2d-4           [-1, 80, 56, 56]          28,880\n",
            "       BatchNorm2d-5           [-1, 80, 56, 56]             160\n",
            "              GELU-6           [-1, 80, 56, 56]               0\n",
            "            Conv2d-7           [-1, 80, 56, 56]          57,680\n",
            "       BatchNorm2d-8           [-1, 80, 56, 56]             160\n",
            "              Stem-9           [-1, 80, 56, 56]               0\n",
            "           Conv2d-10           [-1, 80, 56, 56]           6,480\n",
            "      BatchNorm2d-11           [-1, 80, 56, 56]             160\n",
            "     DenseDilated-12           [-1, 2, 3136, 9]               0\n",
            "DenseDilatedKnnGraph-13           [-1, 2, 3136, 9]               0\n",
            "           Conv2d-14         [-1, 160, 3136, 1]           6,560\n",
            "      BatchNorm2d-15         [-1, 160, 3136, 1]             320\n",
            "             GELU-16         [-1, 160, 3136, 1]               0\n",
            "     Max_relative-17         [-1, 160, 3136, 1]               0\n",
            "    Dynamic_graph-18          [-1, 160, 56, 56]               0\n",
            "           Conv2d-19           [-1, 80, 56, 56]          12,880\n",
            "      BatchNorm2d-20           [-1, 80, 56, 56]             160\n",
            "         Identity-21           [-1, 80, 56, 56]               0\n",
            "          Grapher-22           [-1, 80, 56, 56]               0\n",
            "           Conv2d-23          [-1, 320, 56, 56]          25,920\n",
            "      BatchNorm2d-24          [-1, 320, 56, 56]             640\n",
            "             GELU-25          [-1, 320, 56, 56]               0\n",
            "           Conv2d-26           [-1, 80, 56, 56]          25,680\n",
            "      BatchNorm2d-27           [-1, 80, 56, 56]             160\n",
            "         Identity-28           [-1, 80, 56, 56]               0\n",
            "              FFN-29           [-1, 80, 56, 56]               0\n",
            "           Conv2d-30           [-1, 80, 56, 56]           6,480\n",
            "      BatchNorm2d-31           [-1, 80, 56, 56]             160\n",
            "     DenseDilated-32           [-1, 2, 3136, 9]               0\n",
            "DenseDilatedKnnGraph-33           [-1, 2, 3136, 9]               0\n",
            "           Conv2d-34         [-1, 160, 3136, 1]           6,560\n",
            "      BatchNorm2d-35         [-1, 160, 3136, 1]             320\n",
            "             GELU-36         [-1, 160, 3136, 1]               0\n",
            "     Max_relative-37         [-1, 160, 3136, 1]               0\n",
            "    Dynamic_graph-38          [-1, 160, 56, 56]               0\n",
            "           Conv2d-39           [-1, 80, 56, 56]          12,880\n",
            "      BatchNorm2d-40           [-1, 80, 56, 56]             160\n",
            "         DropPath-41           [-1, 80, 56, 56]               0\n",
            "          Grapher-42           [-1, 80, 56, 56]               0\n",
            "           Conv2d-43          [-1, 320, 56, 56]          25,920\n",
            "      BatchNorm2d-44          [-1, 320, 56, 56]             640\n",
            "             GELU-45          [-1, 320, 56, 56]               0\n",
            "           Conv2d-46           [-1, 80, 56, 56]          25,680\n",
            "      BatchNorm2d-47           [-1, 80, 56, 56]             160\n",
            "         DropPath-48           [-1, 80, 56, 56]               0\n",
            "              FFN-49           [-1, 80, 56, 56]               0\n",
            "           Conv2d-50          [-1, 160, 28, 28]         115,360\n",
            "      BatchNorm2d-51          [-1, 160, 28, 28]             320\n",
            "       Downsample-52          [-1, 160, 28, 28]               0\n",
            "           Conv2d-53          [-1, 160, 28, 28]          25,760\n",
            "      BatchNorm2d-54          [-1, 160, 28, 28]             320\n",
            "     DenseDilated-55            [-1, 2, 784, 9]               0\n",
            "DenseDilatedKnnGraph-56            [-1, 2, 784, 9]               0\n",
            "           Conv2d-57          [-1, 320, 784, 1]          25,920\n",
            "      BatchNorm2d-58          [-1, 320, 784, 1]             640\n",
            "             GELU-59          [-1, 320, 784, 1]               0\n",
            "     Max_relative-60          [-1, 320, 784, 1]               0\n",
            "    Dynamic_graph-61          [-1, 320, 28, 28]               0\n",
            "           Conv2d-62          [-1, 160, 28, 28]          51,360\n",
            "      BatchNorm2d-63          [-1, 160, 28, 28]             320\n",
            "         DropPath-64          [-1, 160, 28, 28]               0\n",
            "          Grapher-65          [-1, 160, 28, 28]               0\n",
            "           Conv2d-66          [-1, 640, 28, 28]         103,040\n",
            "      BatchNorm2d-67          [-1, 640, 28, 28]           1,280\n",
            "             GELU-68          [-1, 640, 28, 28]               0\n",
            "           Conv2d-69          [-1, 160, 28, 28]         102,560\n",
            "      BatchNorm2d-70          [-1, 160, 28, 28]             320\n",
            "         DropPath-71          [-1, 160, 28, 28]               0\n",
            "              FFN-72          [-1, 160, 28, 28]               0\n",
            "           Conv2d-73          [-1, 160, 28, 28]          25,760\n",
            "      BatchNorm2d-74          [-1, 160, 28, 28]             320\n",
            "     DenseDilated-75            [-1, 2, 784, 9]               0\n",
            "DenseDilatedKnnGraph-76            [-1, 2, 784, 9]               0\n",
            "           Conv2d-77          [-1, 320, 784, 1]          25,920\n",
            "      BatchNorm2d-78          [-1, 320, 784, 1]             640\n",
            "             GELU-79          [-1, 320, 784, 1]               0\n",
            "     Max_relative-80          [-1, 320, 784, 1]               0\n",
            "    Dynamic_graph-81          [-1, 320, 28, 28]               0\n",
            "           Conv2d-82          [-1, 160, 28, 28]          51,360\n",
            "      BatchNorm2d-83          [-1, 160, 28, 28]             320\n",
            "         DropPath-84          [-1, 160, 28, 28]               0\n",
            "          Grapher-85          [-1, 160, 28, 28]               0\n",
            "           Conv2d-86          [-1, 640, 28, 28]         103,040\n",
            "      BatchNorm2d-87          [-1, 640, 28, 28]           1,280\n",
            "             GELU-88          [-1, 640, 28, 28]               0\n",
            "           Conv2d-89          [-1, 160, 28, 28]         102,560\n",
            "      BatchNorm2d-90          [-1, 160, 28, 28]             320\n",
            "         DropPath-91          [-1, 160, 28, 28]               0\n",
            "              FFN-92          [-1, 160, 28, 28]               0\n",
            "           Conv2d-93          [-1, 400, 14, 14]         576,400\n",
            "      BatchNorm2d-94          [-1, 400, 14, 14]             800\n",
            "       Downsample-95          [-1, 400, 14, 14]               0\n",
            "           Conv2d-96          [-1, 400, 14, 14]         160,400\n",
            "      BatchNorm2d-97          [-1, 400, 14, 14]             800\n",
            "     DenseDilated-98            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-99            [-1, 2, 196, 9]               0\n",
            "          Conv2d-100          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-101          [-1, 800, 196, 1]           1,600\n",
            "            GELU-102          [-1, 800, 196, 1]               0\n",
            "    Max_relative-103          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-104          [-1, 800, 14, 14]               0\n",
            "          Conv2d-105          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-106          [-1, 400, 14, 14]             800\n",
            "        DropPath-107          [-1, 400, 14, 14]               0\n",
            "         Grapher-108          [-1, 400, 14, 14]               0\n",
            "          Conv2d-109         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-110         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-111         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-112          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-113          [-1, 400, 14, 14]             800\n",
            "        DropPath-114          [-1, 400, 14, 14]               0\n",
            "             FFN-115          [-1, 400, 14, 14]               0\n",
            "          Conv2d-116          [-1, 400, 14, 14]         160,400\n",
            "     BatchNorm2d-117          [-1, 400, 14, 14]             800\n",
            "    DenseDilated-118            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-119            [-1, 2, 196, 9]               0\n",
            "          Conv2d-120          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-121          [-1, 800, 196, 1]           1,600\n",
            "            GELU-122          [-1, 800, 196, 1]               0\n",
            "    Max_relative-123          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-124          [-1, 800, 14, 14]               0\n",
            "          Conv2d-125          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-126          [-1, 400, 14, 14]             800\n",
            "        DropPath-127          [-1, 400, 14, 14]               0\n",
            "         Grapher-128          [-1, 400, 14, 14]               0\n",
            "          Conv2d-129         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-130         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-131         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-132          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-133          [-1, 400, 14, 14]             800\n",
            "        DropPath-134          [-1, 400, 14, 14]               0\n",
            "             FFN-135          [-1, 400, 14, 14]               0\n",
            "          Conv2d-136          [-1, 400, 14, 14]         160,400\n",
            "     BatchNorm2d-137          [-1, 400, 14, 14]             800\n",
            "    DenseDilated-138            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-139            [-1, 2, 196, 9]               0\n",
            "          Conv2d-140          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-141          [-1, 800, 196, 1]           1,600\n",
            "            GELU-142          [-1, 800, 196, 1]               0\n",
            "    Max_relative-143          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-144          [-1, 800, 14, 14]               0\n",
            "          Conv2d-145          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-146          [-1, 400, 14, 14]             800\n",
            "        DropPath-147          [-1, 400, 14, 14]               0\n",
            "         Grapher-148          [-1, 400, 14, 14]               0\n",
            "          Conv2d-149         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-150         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-151         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-152          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-153          [-1, 400, 14, 14]             800\n",
            "        DropPath-154          [-1, 400, 14, 14]               0\n",
            "             FFN-155          [-1, 400, 14, 14]               0\n",
            "          Conv2d-156          [-1, 400, 14, 14]         160,400\n",
            "     BatchNorm2d-157          [-1, 400, 14, 14]             800\n",
            "    DenseDilated-158            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-159            [-1, 2, 196, 9]               0\n",
            "          Conv2d-160          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-161          [-1, 800, 196, 1]           1,600\n",
            "            GELU-162          [-1, 800, 196, 1]               0\n",
            "    Max_relative-163          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-164          [-1, 800, 14, 14]               0\n",
            "          Conv2d-165          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-166          [-1, 400, 14, 14]             800\n",
            "        DropPath-167          [-1, 400, 14, 14]               0\n",
            "         Grapher-168          [-1, 400, 14, 14]               0\n",
            "          Conv2d-169         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-170         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-171         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-172          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-173          [-1, 400, 14, 14]             800\n",
            "        DropPath-174          [-1, 400, 14, 14]               0\n",
            "             FFN-175          [-1, 400, 14, 14]               0\n",
            "          Conv2d-176          [-1, 400, 14, 14]         160,400\n",
            "     BatchNorm2d-177          [-1, 400, 14, 14]             800\n",
            "    DenseDilated-178            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-179            [-1, 2, 196, 9]               0\n",
            "          Conv2d-180          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-181          [-1, 800, 196, 1]           1,600\n",
            "            GELU-182          [-1, 800, 196, 1]               0\n",
            "    Max_relative-183          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-184          [-1, 800, 14, 14]               0\n",
            "          Conv2d-185          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-186          [-1, 400, 14, 14]             800\n",
            "        DropPath-187          [-1, 400, 14, 14]               0\n",
            "         Grapher-188          [-1, 400, 14, 14]               0\n",
            "          Conv2d-189         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-190         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-191         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-192          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-193          [-1, 400, 14, 14]             800\n",
            "        DropPath-194          [-1, 400, 14, 14]               0\n",
            "             FFN-195          [-1, 400, 14, 14]               0\n",
            "          Conv2d-196          [-1, 400, 14, 14]         160,400\n",
            "     BatchNorm2d-197          [-1, 400, 14, 14]             800\n",
            "    DenseDilated-198            [-1, 2, 196, 9]               0\n",
            "DenseDilatedKnnGraph-199            [-1, 2, 196, 9]               0\n",
            "          Conv2d-200          [-1, 800, 196, 1]         160,800\n",
            "     BatchNorm2d-201          [-1, 800, 196, 1]           1,600\n",
            "            GELU-202          [-1, 800, 196, 1]               0\n",
            "    Max_relative-203          [-1, 800, 196, 1]               0\n",
            "   Dynamic_graph-204          [-1, 800, 14, 14]               0\n",
            "          Conv2d-205          [-1, 400, 14, 14]         320,400\n",
            "     BatchNorm2d-206          [-1, 400, 14, 14]             800\n",
            "        DropPath-207          [-1, 400, 14, 14]               0\n",
            "         Grapher-208          [-1, 400, 14, 14]               0\n",
            "          Conv2d-209         [-1, 1600, 14, 14]         641,600\n",
            "     BatchNorm2d-210         [-1, 1600, 14, 14]           3,200\n",
            "            GELU-211         [-1, 1600, 14, 14]               0\n",
            "          Conv2d-212          [-1, 400, 14, 14]         640,400\n",
            "     BatchNorm2d-213          [-1, 400, 14, 14]             800\n",
            "        DropPath-214          [-1, 400, 14, 14]               0\n",
            "             FFN-215          [-1, 400, 14, 14]               0\n",
            "          Conv2d-216            [-1, 640, 7, 7]       2,304,640\n",
            "     BatchNorm2d-217            [-1, 640, 7, 7]           1,280\n",
            "      Downsample-218            [-1, 640, 7, 7]               0\n",
            "          Conv2d-219            [-1, 640, 7, 7]         410,240\n",
            "     BatchNorm2d-220            [-1, 640, 7, 7]           1,280\n",
            "    DenseDilated-221             [-1, 2, 49, 9]               0\n",
            "DenseDilatedKnnGraph-222             [-1, 2, 49, 9]               0\n",
            "          Conv2d-223          [-1, 1280, 49, 1]         410,880\n",
            "     BatchNorm2d-224          [-1, 1280, 49, 1]           2,560\n",
            "            GELU-225          [-1, 1280, 49, 1]               0\n",
            "    Max_relative-226          [-1, 1280, 49, 1]               0\n",
            "   Dynamic_graph-227           [-1, 1280, 7, 7]               0\n",
            "          Conv2d-228            [-1, 640, 7, 7]         819,840\n",
            "     BatchNorm2d-229            [-1, 640, 7, 7]           1,280\n",
            "        DropPath-230            [-1, 640, 7, 7]               0\n",
            "         Grapher-231            [-1, 640, 7, 7]               0\n",
            "          Conv2d-232           [-1, 2560, 7, 7]       1,640,960\n",
            "     BatchNorm2d-233           [-1, 2560, 7, 7]           5,120\n",
            "            GELU-234           [-1, 2560, 7, 7]               0\n",
            "          Conv2d-235            [-1, 640, 7, 7]       1,639,040\n",
            "     BatchNorm2d-236            [-1, 640, 7, 7]           1,280\n",
            "        DropPath-237            [-1, 640, 7, 7]               0\n",
            "             FFN-238            [-1, 640, 7, 7]               0\n",
            "          Conv2d-239            [-1, 640, 7, 7]         410,240\n",
            "     BatchNorm2d-240            [-1, 640, 7, 7]           1,280\n",
            "    DenseDilated-241             [-1, 2, 49, 9]               0\n",
            "DenseDilatedKnnGraph-242             [-1, 2, 49, 9]               0\n",
            "          Conv2d-243          [-1, 1280, 49, 1]         410,880\n",
            "     BatchNorm2d-244          [-1, 1280, 49, 1]           2,560\n",
            "            GELU-245          [-1, 1280, 49, 1]               0\n",
            "    Max_relative-246          [-1, 1280, 49, 1]               0\n",
            "   Dynamic_graph-247           [-1, 1280, 7, 7]               0\n",
            "          Conv2d-248            [-1, 640, 7, 7]         819,840\n",
            "     BatchNorm2d-249            [-1, 640, 7, 7]           1,280\n",
            "        DropPath-250            [-1, 640, 7, 7]               0\n",
            "         Grapher-251            [-1, 640, 7, 7]               0\n",
            "          Conv2d-252           [-1, 2560, 7, 7]       1,640,960\n",
            "     BatchNorm2d-253           [-1, 2560, 7, 7]           5,120\n",
            "            GELU-254           [-1, 2560, 7, 7]               0\n",
            "          Conv2d-255            [-1, 640, 7, 7]       1,639,040\n",
            "     BatchNorm2d-256            [-1, 640, 7, 7]           1,280\n",
            "        DropPath-257            [-1, 640, 7, 7]               0\n",
            "             FFN-258            [-1, 640, 7, 7]               0\n",
            "          Conv2d-259           [-1, 1024, 1, 1]         656,384\n",
            "     BatchNorm2d-260           [-1, 1024, 1, 1]           2,048\n",
            "            GELU-261           [-1, 1024, 1, 1]               0\n",
            "         Dropout-262           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-263              [-1, 6, 1, 1]           6,150\n",
            "================================================================\n",
            "Total params: 25,982,182\n",
            "Trainable params: 25,982,182\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 344.79\n",
            "Params size (MB): 99.11\n",
            "Estimated Total Size (MB): 444.48\n",
            "----------------------------------------------------------------\n",
            "28005000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "input_size = (3,224,224)\n",
        "if isinstance(model, torch.nn.DataParallel):\n",
        "    model = model.module\n",
        "\n",
        "summary(model, input_size)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW45ib6S70NJ"
      },
      "outputs": [],
      "source": [
        "example_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust the size based on your model's input\n",
        "traced_script_module = torch.jit.trace(model, example_input)\n",
        "#scripted_module = torch.jit.script(model)\n",
        "traced_script_module.save(\"traced_model.pt\")\n",
        "# OR\n",
        "# scripted_module.save(\"scripted_model.pt\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGxNt1qt70NK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUxxPoSS70NK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU25kehE70NK",
        "outputId": "7fad104c-5334-48dd-f968-2c49e0997c42"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (4204047870.py, line 30)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[43], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    running_loss = 0.0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "# Define the model\n",
        "model = pvig_ti_224_gelu(num_classes=6).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "#for epoch in range(300):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, data in enumerate(balanced_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 25== 24:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}%')\n",
        "\n",
        "print('Finished Training')\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, 'model_checkpoint.pth')\n",
        "\n",
        "\n",
        "# Now let's plot the training loss and accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, 'r-', label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, 'g-', label='Train Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw781Rq070NK",
        "outputId": "25b989db-2e10-46c6-9725-ad067d1eeb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function check_balance at 0x7f9641c8be20>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0: 94, 1: 168, 2: 158, 3: 71, 4: 117, 5: 30}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_count_test=check_balance(test_loader)\n",
        "class_count_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVUYNalJ70NK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "target_samples_per_class_test = max(class_count_test.values())\n",
        "\n",
        "\n",
        "indices_per_class_test = {i: [] for i in class_count.keys()}\n",
        "for idx, (_, label) in enumerate(test_dataset):\n",
        "    indices_per_class_test[label].append(idx)\n",
        "\n",
        "\n",
        "datasets_test = []\n",
        "\n",
        "\n",
        "for class_label, indices in indices_per_class_test.items():\n",
        "    # Calculate the number of extra samples needed to reach the target\n",
        "    extra_samples = target_samples_per_class_test - len(indices)\n",
        "\n",
        "\n",
        "    class_subset = Subset(test_dataset, indices)\n",
        "\n",
        "\n",
        "    transform = transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "  # Resize all images to 64x64\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "    class_subset.dataset.transform = transform\n",
        "\n",
        "\n",
        "    class_datasets = [class_subset] * (1 + extra_samples // len(indices))\n",
        "\n",
        "\n",
        "    if extra_samples % len(indices) != 0:\n",
        "        extra_subset = Subset(class_subset, range(extra_samples % len(indices)))\n",
        "        class_datasets.append(extra_subset)\n",
        "\n",
        "    # Concatenate the class datasets together\n",
        "    class_dataset = ConcatDataset(class_datasets)\n",
        "\n",
        "    # Add the class dataset to the list of datasets\n",
        "    datasets_test.append(class_dataset)\n",
        "\n",
        "# Concatenate all the class datasets together\n",
        "balanced_dataset_test = ConcatDataset(datasets_test)\n",
        "\n",
        "# Create a new DataLoader for the balanced dataset\n",
        "balanced_train_loader_test = DataLoader(balanced_dataset_test, batch_size=46, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPmu0JSJ70NL",
        "outputId": "03e42507-3ba7-4f2c-de12-c91b28e4590d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function check_balance at 0x7f9641c8be20>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{5: 168, 1: 168, 3: 168, 0: 168, 2: 168, 4: 168}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "check_balance(balanced_train_loader_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG8gEUT770NL",
        "outputId": "ba9407e5-ddea-48b0-c870-01b42e916346"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ba' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check_balance(ba)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ba' is not defined"
          ]
        }
      ],
      "source": [
        "check_balance(ba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.pvig_ti_224_gelu.<locals>.OptInit object at 0x7f3960482800>\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 6.30957344e-01 3.98107171e-01 ... 3.98107171e-04\n",
            "  2.51188643e-04 1.58489319e-04]\n",
            " [2.00000000e+00 1.26191469e+00 7.96214341e-01 ... 7.96214341e-04\n",
            "  5.02377286e-04 3.16978638e-04]\n",
            " ...\n",
            " [5.30000000e+01 3.34407393e+01 2.10996800e+01 ... 2.10996800e-02\n",
            "  1.33129981e-02 8.39993392e-03]\n",
            " [5.40000000e+01 3.40716966e+01 2.14977872e+01 ... 2.14977872e-02\n",
            "  1.35641867e-02 8.55842324e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 6.30957344e-01 3.98107171e-01 ... 3.98107171e-04\n",
            "  2.51188643e-04 1.58489319e-04]\n",
            " [2.00000000e+00 1.26191469e+00 7.96214341e-01 ... 7.96214341e-04\n",
            "  5.02377286e-04 3.16978638e-04]\n",
            " ...\n",
            " [5.30000000e+01 3.34407393e+01 2.10996800e+01 ... 2.10996800e-02\n",
            "  1.33129981e-02 8.39993392e-03]\n",
            " [5.40000000e+01 3.40716966e+01 2.14977872e+01 ... 2.14977872e-02\n",
            "  1.35641867e-02 8.55842324e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19.]\n",
            "[1.00000000e+00 6.30957344e-01 3.98107171e-01 2.51188643e-01\n",
            " 1.58489319e-01 1.00000000e-01 6.30957344e-02 3.98107171e-02\n",
            " 2.51188643e-02 1.58489319e-02 1.00000000e-02 6.30957344e-03\n",
            " 3.98107171e-03 2.51188643e-03 1.58489319e-03 1.00000000e-03\n",
            " 6.30957344e-04 3.98107171e-04 2.51188643e-04 1.58489319e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]\n",
            " [5.50000000e+01 3.47026539e+01 2.18958944e+01 ... 2.18958944e-02\n",
            "  1.38153754e-02 8.71691256e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 7.94328235e-01 6.30957344e-01 ... 1.99526231e-04\n",
            "  1.58489319e-04 1.25892541e-04]\n",
            " [2.00000000e+00 1.58865647e+00 1.26191469e+00 ... 3.99052463e-04\n",
            "  3.16978638e-04 2.51785082e-04]\n",
            " ...\n",
            " [2.50000000e+01 1.98582059e+01 1.57739336e+01 ... 4.98815579e-03\n",
            "  3.96223298e-03 3.14731353e-03]\n",
            " [2.60000000e+01 2.06525341e+01 1.64048910e+01 ... 5.18768202e-03\n",
            "  4.12072230e-03 3.27320607e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 7.94328235e-01 6.30957344e-01 ... 1.99526231e-04\n",
            "  1.58489319e-04 1.25892541e-04]\n",
            " [2.00000000e+00 1.58865647e+00 1.26191469e+00 ... 3.99052463e-04\n",
            "  3.16978638e-04 2.51785082e-04]\n",
            " ...\n",
            " [2.50000000e+01 1.98582059e+01 1.57739336e+01 ... 4.98815579e-03\n",
            "  3.96223298e-03 3.14731353e-03]\n",
            " [2.60000000e+01 2.06525341e+01 1.64048910e+01 ... 5.18768202e-03\n",
            "  4.12072230e-03 3.27320607e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39.]\n",
            "[1.00000000e+00 7.94328235e-01 6.30957344e-01 5.01187234e-01\n",
            " 3.98107171e-01 3.16227766e-01 2.51188643e-01 1.99526231e-01\n",
            " 1.58489319e-01 1.25892541e-01 1.00000000e-01 7.94328235e-02\n",
            " 6.30957344e-02 5.01187234e-02 3.98107171e-02 3.16227766e-02\n",
            " 2.51188643e-02 1.99526231e-02 1.58489319e-02 1.25892541e-02\n",
            " 1.00000000e-02 7.94328235e-03 6.30957344e-03 5.01187234e-03\n",
            " 3.98107171e-03 3.16227766e-03 2.51188643e-03 1.99526231e-03\n",
            " 1.58489319e-03 1.25892541e-03 1.00000000e-03 7.94328235e-04\n",
            " 6.30957344e-04 5.01187234e-04 3.98107171e-04 3.16227766e-04\n",
            " 2.51188643e-04 1.99526231e-04 1.58489319e-04 1.25892541e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]\n",
            " [2.70000000e+01 2.14468623e+01 1.70358483e+01 ... 5.38720825e-03\n",
            "  4.27921162e-03 3.39909861e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.12010839e-01 8.31763771e-01 ... 1.31825674e-04\n",
            "  1.20226443e-04 1.09647820e-04]\n",
            " [2.00000000e+00 1.82402168e+00 1.66352754e+00 ... 2.63651348e-04\n",
            "  2.40452887e-04 2.19295639e-04]\n",
            " ...\n",
            " [1.10000000e+01 1.00321192e+01 9.14940148e+00 ... 1.45008241e-03\n",
            "  1.32249088e-03 1.20612602e-03]\n",
            " [1.20000000e+01 1.09441301e+01 9.98116525e+00 ... 1.58190809e-03\n",
            "  1.44271732e-03 1.31577384e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.\n",
            " 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
            "[1.00000000e+00 9.12010839e-01 8.31763771e-01 7.58577575e-01\n",
            " 6.91830971e-01 6.30957344e-01 5.75439937e-01 5.24807460e-01\n",
            " 4.78630092e-01 4.36515832e-01 3.98107171e-01 3.63078055e-01\n",
            " 3.31131121e-01 3.01995172e-01 2.75422870e-01 2.51188643e-01\n",
            " 2.29086765e-01 2.08929613e-01 1.90546072e-01 1.73780083e-01\n",
            " 1.58489319e-01 1.44543977e-01 1.31825674e-01 1.20226443e-01\n",
            " 1.09647820e-01 1.00000000e-01 9.12010839e-02 8.31763771e-02\n",
            " 7.58577575e-02 6.91830971e-02 6.30957344e-02 5.75439937e-02\n",
            " 5.24807460e-02 4.78630092e-02 4.36515832e-02 3.98107171e-02\n",
            " 3.63078055e-02 3.31131121e-02 3.01995172e-02 2.75422870e-02\n",
            " 2.51188643e-02 2.29086765e-02 2.08929613e-02 1.90546072e-02\n",
            " 1.73780083e-02 1.58489319e-02 1.44543977e-02 1.31825674e-02\n",
            " 1.20226443e-02 1.09647820e-02 1.00000000e-02 9.12010839e-03\n",
            " 8.31763771e-03 7.58577575e-03 6.91830971e-03 6.30957344e-03\n",
            " 5.75439937e-03 5.24807460e-03 4.78630092e-03 4.36515832e-03\n",
            " 3.98107171e-03 3.63078055e-03 3.31131121e-03 3.01995172e-03\n",
            " 2.75422870e-03 2.51188643e-03 2.29086765e-03 2.08929613e-03\n",
            " 1.90546072e-03 1.73780083e-03 1.58489319e-03 1.44543977e-03\n",
            " 1.31825674e-03 1.20226443e-03 1.09647820e-03 1.00000000e-03\n",
            " 9.12010839e-04 8.31763771e-04 7.58577575e-04 6.91830971e-04\n",
            " 6.30957344e-04 5.75439937e-04 5.24807460e-04 4.78630092e-04\n",
            " 4.36515832e-04 3.98107171e-04 3.63078055e-04 3.31131121e-04\n",
            " 3.01995172e-04 2.75422870e-04 2.51188643e-04 2.29086765e-04\n",
            " 2.08929613e-04 1.90546072e-04 1.73780083e-04 1.58489319e-04\n",
            " 1.44543977e-04 1.31825674e-04 1.20226443e-04 1.09647820e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]\n",
            " [1.30000000e+01 1.18561409e+01 1.08129290e+01 ... 1.71373376e-03\n",
            "  1.56294377e-03 1.42542165e-03]]\n",
            "using relative pos\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.44060876e-01 8.91250938e-01 ... 1.18850223e-04\n",
            "  1.12201845e-04 1.05925373e-04]\n",
            " [2.00000000e+00 1.88812175e+00 1.78250188e+00 ... 2.37700445e-04\n",
            "  2.24403691e-04 2.11850745e-04]\n",
            " ...\n",
            " [4.00000000e+00 3.77624351e+00 3.56500375e+00 ... 4.75400891e-04\n",
            "  4.48807382e-04 4.23701490e-04]\n",
            " [5.00000000e+00 4.72030438e+00 4.45625469e+00 ... 5.94251114e-04\n",
            "  5.61009227e-04 5.29626863e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "using relative pos\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 9.44060876e-01 8.91250938e-01 ... 1.18850223e-04\n",
            "  1.12201845e-04 1.05925373e-04]\n",
            " [2.00000000e+00 1.88812175e+00 1.78250188e+00 ... 2.37700445e-04\n",
            "  2.24403691e-04 2.11850745e-04]\n",
            " ...\n",
            " [4.00000000e+00 3.77624351e+00 3.56500375e+00 ... 4.75400891e-04\n",
            "  4.48807382e-04 4.23701490e-04]\n",
            " [5.00000000e+00 4.72030438e+00 4.45625469e+00 ... 5.94251114e-04\n",
            "  5.61009227e-04 5.29626863e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n",
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
            " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
            " 154. 155. 156. 157. 158. 159.]\n",
            "[1.00000000e+00 9.44060876e-01 8.91250938e-01 8.41395142e-01\n",
            " 7.94328235e-01 7.49894209e-01 7.07945784e-01 6.68343918e-01\n",
            " 6.30957344e-01 5.95662144e-01 5.62341325e-01 5.30884444e-01\n",
            " 5.01187234e-01 4.73151259e-01 4.46683592e-01 4.21696503e-01\n",
            " 3.98107171e-01 3.75837404e-01 3.54813389e-01 3.34965439e-01\n",
            " 3.16227766e-01 2.98538262e-01 2.81838293e-01 2.66072506e-01\n",
            " 2.51188643e-01 2.37137371e-01 2.23872114e-01 2.11348904e-01\n",
            " 1.99526231e-01 1.88364909e-01 1.77827941e-01 1.67880402e-01\n",
            " 1.58489319e-01 1.49623566e-01 1.41253754e-01 1.33352143e-01\n",
            " 1.25892541e-01 1.18850223e-01 1.12201845e-01 1.05925373e-01\n",
            " 1.00000000e-01 9.44060876e-02 8.91250938e-02 8.41395142e-02\n",
            " 7.94328235e-02 7.49894209e-02 7.07945784e-02 6.68343918e-02\n",
            " 6.30957344e-02 5.95662144e-02 5.62341325e-02 5.30884444e-02\n",
            " 5.01187234e-02 4.73151259e-02 4.46683592e-02 4.21696503e-02\n",
            " 3.98107171e-02 3.75837404e-02 3.54813389e-02 3.34965439e-02\n",
            " 3.16227766e-02 2.98538262e-02 2.81838293e-02 2.66072506e-02\n",
            " 2.51188643e-02 2.37137371e-02 2.23872114e-02 2.11348904e-02\n",
            " 1.99526231e-02 1.88364909e-02 1.77827941e-02 1.67880402e-02\n",
            " 1.58489319e-02 1.49623566e-02 1.41253754e-02 1.33352143e-02\n",
            " 1.25892541e-02 1.18850223e-02 1.12201845e-02 1.05925373e-02\n",
            " 1.00000000e-02 9.44060876e-03 8.91250938e-03 8.41395142e-03\n",
            " 7.94328235e-03 7.49894209e-03 7.07945784e-03 6.68343918e-03\n",
            " 6.30957344e-03 5.95662144e-03 5.62341325e-03 5.30884444e-03\n",
            " 5.01187234e-03 4.73151259e-03 4.46683592e-03 4.21696503e-03\n",
            " 3.98107171e-03 3.75837404e-03 3.54813389e-03 3.34965439e-03\n",
            " 3.16227766e-03 2.98538262e-03 2.81838293e-03 2.66072506e-03\n",
            " 2.51188643e-03 2.37137371e-03 2.23872114e-03 2.11348904e-03\n",
            " 1.99526231e-03 1.88364909e-03 1.77827941e-03 1.67880402e-03\n",
            " 1.58489319e-03 1.49623566e-03 1.41253754e-03 1.33352143e-03\n",
            " 1.25892541e-03 1.18850223e-03 1.12201845e-03 1.05925373e-03\n",
            " 1.00000000e-03 9.44060876e-04 8.91250938e-04 8.41395142e-04\n",
            " 7.94328235e-04 7.49894209e-04 7.07945784e-04 6.68343918e-04\n",
            " 6.30957344e-04 5.95662144e-04 5.62341325e-04 5.30884444e-04\n",
            " 5.01187234e-04 4.73151259e-04 4.46683592e-04 4.21696503e-04\n",
            " 3.98107171e-04 3.75837404e-04 3.54813389e-04 3.34965439e-04\n",
            " 3.16227766e-04 2.98538262e-04 2.81838293e-04 2.66072506e-04\n",
            " 2.51188643e-04 2.37137371e-04 2.23872114e-04 2.11348904e-04\n",
            " 1.99526231e-04 1.88364909e-04 1.77827941e-04 1.67880402e-04\n",
            " 1.58489319e-04 1.49623566e-04 1.41253754e-04 1.33352143e-04\n",
            " 1.25892541e-04 1.18850223e-04 1.12201845e-04 1.05925373e-04]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]\n",
            " [6.00000000e+00 5.66436526e+00 5.34750563e+00 ... 7.13101336e-04\n",
            "  6.73211073e-04 6.35552235e-04]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = pvig_ti_224_gelu(num_classes=6).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXLPPWeF70NL",
        "outputId": "8d166c07-b9fe-406d-dadc-95ed872a6745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 80 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load('model_checkpoint_ocast.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in balanced_train_loader_test:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss-4-p_N70NL"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[138], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodel_checkpoint.pth\u001b[39;49m\u001b[39m'\u001b[39;49m,map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      9\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1079\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1083\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1084\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1055\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, \u001b[39mstr\u001b[39;49m(map_location))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:80\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\n\u001b[1;32m     81\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load('model_checkpoint.pth',map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "accuracies = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in balanced_train_loader_test:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Compute the current accuracy and append it to the list\n",
        "        current_accuracy = 100 * correct / total\n",
        "        accuracies.append(current_accuracy)\n",
        "\n",
        "print('Final accuracy of the network on the test images: %d %%' % accuracies[-1])\n",
        "\n",
        "# Plot the accuracies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(accuracies, label='Testing accuracy')\n",
        "plt.title('Accuracy over time')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final accuracy of the network on the test images: 82 %\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "with open('model_ocast.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "model_1 = loaded_model.to(device)\n",
        "model_1.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "accuracies = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model_1(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Compute the current accuracy and append it to the list\n",
        "        current_accuracy = 100 * correct / total\n",
        "        accuracies.append(current_accuracy)\n",
        "\n",
        "print('Final accuracy of the network on the test images: %d %%' % accuracies[-1])\n",
        "\n",
        "# Plot the accuracies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(accuracies, label='Testing accuracy')\n",
        "plt.title('Accuracy over time')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
